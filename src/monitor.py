import requests
import json
import os
from datetime import datetime, timezone
import cloudscraper
import time
import traceback
import random
from urllib.parse import urlencode

class CloudflareBypasser:
    def __init__(self):
        # å˜—è©¦å¤šç¨®ä¸åŒçš„ cloudscraper é…ç½®
        self.scrapers = []
        
        # é…ç½® 1: Chrome + Windows
        scraper1 = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            debug=False,
            delay=random.uniform(5, 10)
        )
        self.scrapers.append(('Chrome-Windows', scraper1))
        
        # é…ç½® 2: Firefox + Linux
        scraper2 = cloudscraper.create_scraper(
            browser={
                'browser': 'firefox',
                'platform': 'linux',
                'mobile': False
            },
            debug=False
        )
        self.scrapers.append(('Firefox-Linux', scraper2))
        
        # é…ç½® 3: Chrome + Mac
        scraper3 = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'darwin',
                'mobile': False
            },
            debug=False
        )
        self.scrapers.append(('Chrome-Mac', scraper3))
        
        # é…ç½® 4: æ‰‹å‹•è¨­å®š headers
        scraper4 = requests.Session()
        scraper4.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        })
        self.scrapers.append(('Manual-Headers', scraper4))
        
        self.keywords = self.load_keywords()
        self.base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.results_dir = os.path.join(self.base_dir, "results")
        self.ensure_results_dir()
        
        # Telegram è¨­å®š
        self.telegram_token = os.environ.get('TELEGRAM_BOT_TOKEN')
        self.telegram_chat_id = os.environ.get('TELEGRAM_CHAT_ID')
        
        print(f"ğŸ”§ åˆå§‹åŒ–å®Œæˆï¼Œæº–å‚™äº† {len(self.scrapers)} ç¨®ç¹éæ–¹æ³•")
        
    def load_keywords(self):
        return [
            "é‡‘å·¥", "éŠ€å·¥", "æ‰‹ä½œé‡‘å·¥", "é‡‘å·¥æ•™å­¸", "é‡‘å·¥èª²ç¨‹", "é‡‘å·¥å·¥ä½œå®¤",
            "ç å¯¶", "ç å¯¶è¨­è¨ˆ", "ç å¯¶è£½ä½œ", "é¦–é£¾", "é¦–é£¾è¨­è¨ˆ", "æ‰‹ä½œé¦–é£¾",
            "é‘²åµŒ", "å¯¶çŸ³é‘²åµŒ", "ç¶­ä¿®", "ç å¯¶ç¶­ä¿®", "é¦–é£¾ç¶­ä¿®", "æ”¹åœ",
            "æ‹‹å…‰", "é›»é", "ç„Šæ¥", "é›•è Ÿ", "é‘„é€ ",
            "Ké‡‘", "18K", "14K", "ç™½é‡‘", "é»ƒé‡‘", "ç«ç‘°é‡‘", "ç´”éŠ€", "925éŠ€",
            "é‘½çŸ³", "å¯¶çŸ³", "ç¿¡ç¿ ", "çç ", "ç´…å¯¶çŸ³", "è—å¯¶çŸ³", "ç¥–æ¯ç¶ ",
            "æˆ’æŒ‡", "é …éŠ", "æ‰‹éŠ", "è€³ç’°", "å©šæˆ’", "å°æˆ’", "æ±‚å©šæˆ’æŒ‡", "æƒ…ä¾¶æˆ’",
            "è¨‚åš", "å®¢è£½", "è¨‚è£½", "æ¨è–¦", "åˆ†äº«", "è©•åƒ¹", "é–‹ç®±"
        ]
    
    def ensure_results_dir(self):
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
    
    def try_alternative_endpoints(self, forum):
        """å˜—è©¦ä¸åŒçš„ API ç«¯é»"""
        endpoints = [
            f"https://www.dcard.tw/service/api/v2/forums/{forum}/posts",
            f"https://www.dcard.tw/_api/forums/{forum}/posts",  # èˆŠç‰ˆ API
            f"https://dcard.tw/service/api/v2/forums/{forum}/posts"  # ç„¡ www
        ]
        
        return endpoints
    
    def get_dcard_posts_with_multiple_methods(self, forum, limit=20):
        """ä½¿ç”¨å¤šç¨®æ–¹æ³•å˜—è©¦ç²å–æ–‡ç« """
        endpoints = self.try_alternative_endpoints(forum)
        
        for scraper_name, scraper in self.scrapers:
            print(f"ğŸ”„ ä½¿ç”¨ {scraper_name} æ–¹æ³•...")
            
            for endpoint in endpoints:
                try:
                    print(f"ğŸ“¡ å˜—è©¦ç«¯é»: {endpoint}")
                    
                    # éš¨æ©Ÿå»¶é²
                    delay = random.uniform(3, 8)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)
                    
                    params = {
                        'limit': limit,
                        'popular': 'false'
                    }
                    
                    # è¨­å®šç‰¹å®šçš„ headers
                    if hasattr(scraper, 'headers'):
                        scraper.headers.update({
                            'Referer': f'https://www.dcard.tw/f/{forum}',
                            'Origin': 'https://www.dcard.tw',
                        })
                    
                    response = scraper.get(
                        endpoint,
                        params=params,
                        timeout=30,
                        allow_redirects=True
                    )
                    
                    print(f"ğŸ“Š ç‹€æ…‹ç¢¼: {response.status_code}")
                    print(f"ğŸ“„ å›æ‡‰é•·åº¦: {len(response.text)} å­—å…ƒ")
                    
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if isinstance(data, list) and len(data) > 0:
                                print(f"âœ… æˆåŠŸ! ç²å– {len(data)} ç¯‡æ–‡ç« ")
                                return data
                            else:
                                print("âš ï¸ å›æ‡‰æ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©º")
                        except json.JSONDecodeError as e:
                            print(f"ğŸ“„ JSON è§£æå¤±æ•—: {e}")
                            print(f"å›æ‡‰å…§å®¹å‰ 200 å­—å…ƒ: {response.text[:200]}")
                    
                    elif response.status_code == 403:
                        print("ğŸš« 403 Forbidden - ç¹¼çºŒå˜—è©¦å…¶ä»–æ–¹æ³•")
                    else:
                        print(f"âš ï¸ ç‹€æ…‹ç¢¼ {response.status_code}")
                        
                except requests.exceptions.RequestException as e:
                    print(f"ğŸŒ ç¶²è·¯éŒ¯èª¤: {e}")
                except Exception as e:
                    print(f"âŒ æœªçŸ¥éŒ¯èª¤: {e}")
                
                # æ¯æ¬¡å˜—è©¦å¾Œç­‰å¾…
                time.sleep(random.uniform(2, 5))
            
            # æ¯å€‹ scraper ä¹‹é–“ç­‰å¾…æ›´é•·æ™‚é–“
            if scraper_name != self.scrapers[-1][0]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹
                wait_time = random.uniform(10, 15)
                print(f"â³ åˆ‡æ›æ–¹æ³•å‰ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
        
        print(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—äº†ï¼Œç„¡æ³•ç²å– {forum} ç‰ˆæ–‡ç« ")
        return []
    
    def check_keywords(self, text):
        if not text:
            return []
        text_lower = text.lower()
        matched = []
        for keyword in self.keywords:
            if keyword.lower() in text_lower:
                matched.append(keyword)
        return matched
    
    def save_match(self, post, forum, forum_name, keywords):
        now = datetime.now(timezone.utc)
        taiwan_time = now.replace(tzinfo=timezone.utc).astimezone(tz=None)
        
        url = f"https://www.dcard.tw/f/{forum}/p/{post['id']}"
        
        match_data = {
            'id': post['id'],
            'forum': forum,
            'forum_name': forum_name,
            'title': post.get('title', ''),
            'url': url,
            'author': f"{post.get('school', '')} {post.get('department', '')}".strip(),
            'matched_keywords': keywords,
            'excerpt': post.get('excerpt', '')[:200],
            'like_count': post.get('likeCount', 0),
            'comment_count': post.get('commentCount', 0),
            'created_at': post.get('createdAt', ''),
            'found_at': taiwan_time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜åˆ°æª”æ¡ˆ
        today = taiwan_time.strftime('%Y-%m-%d')
        json_file = os.path.join(self.results_dir, f"matches_{today}.json")
        
        matches = []
        if os.path.exists(json_file):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    matches = json.load(f)
            except:
                matches = []
        
        matches.append(match_data)
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(matches, f, ensure_ascii=False, indent=2)
        
        summary_file = os.path.join(self.base_dir, "latest_matches.txt")
        with open(summary_file, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*60}\n")
            f.write(f"ç™¼ç¾æ™‚é–“: {match_data['found_at']} (å°ç£æ™‚é–“)\n")
            f.write(f"å¹³å°: Dcard {forum_name}\n")
            f.write(f"æ¨™é¡Œ: {match_data['title']}\n")
            f.write(f"ç¶²å€: {url}\n")
            f.write(f"åŒ¹é…é—œéµå­—: {', '.join(keywords)}\n")
        
        print(f"âœ… ä¿å­˜åŒ¹é…: {post.get('title', '')[:50]}...")
        return match_data
    
    def run_monitoring(self):
        start_time = datetime.now()
        print("ğŸš€ é–‹å§‹å¤šæ–¹æ³• Cloudflare ç¹éç›£æ§")
        print(f"â° é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        forums = {
            'jewelry': 'ç å¯¶ç‰ˆ',
            'marriage': 'çµå©šç‰ˆ',
            'girl': 'å¥³å­©ç‰ˆ'
        }
        
        all_matches = []
        successful_forums = 0
        
        for forum_key, forum_name in forums.items():
            print(f"\n{'='*60}")
            print(f"ğŸ“± é–‹å§‹è™•ç† {forum_name}")
            print(f"{'='*60}")
            
            try:
                posts = self.get_dcard_posts_with_multiple_methods(forum_key, limit=20)
                
                if posts:
                    successful_forums += 1
                    print(f"ğŸ‰ {forum_name} æˆåŠŸç²å– {len(posts)} ç¯‡æ–‡ç« ")
                    
                    matches = []
                    for post in posts:
                        title = post.get('title', '')
                        excerpt = post.get('excerpt', '')
                        text = f"{title} {excerpt}"
                        
                        matched_keywords = self.check_keywords(text)
                        
                        if matched_keywords:
                            match_data = self.save_match(post, forum_key, forum_name, matched_keywords)
                            matches.append(match_data)
                    
                    all_matches.extend(matches)
                    print(f"âœ… {forum_name} è™•ç†å®Œæˆï¼Œç™¼ç¾ {len(matches)} ç¯‡åŒ¹é…")
                else:
                    print(f"âŒ {forum_name} ç„¡æ³•ç²å–æ–‡ç« ")
                
            except Exception as e:
                print(f"âŒ è™•ç† {forum_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # è«–å£‡é–“ç­‰å¾…
            if forum_key != list(forums.keys())[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹è«–å£‡
                wait_time = random.uniform(15, 25)
                print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹è«–å£‡...")
                time.sleep(wait_time)
        
        # çµæœæ‘˜è¦
        end_time = datetime.now()
        duration = (end_time - start_time).seconds
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ ç›£æ§ä»»å‹™å®Œæˆ!")
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“: {duration} ç§’")
        print(f"ğŸ“Š æˆåŠŸè«–å£‡: {successful_forums}/{len(forums)}")
        print(f"ğŸ¯ ç¸½åŒ¹é…æ–‡ç« : {len(all_matches)} ç¯‡")
        
        if all_matches:
            print(f"\nğŸ† åŒ¹é…çµæœ:")
            for match in all_matches:
                print(f"â€¢ {match['forum_name']}: {match['title'][:40]}...")
        else:
            print("âœ… æœ¬æ¬¡æœªç™¼ç¾åŒ¹é…æ–‡ç« ï¼ˆæˆ–ç„¡æ³•ç²å–æ–‡ç« ï¼‰")
        
        # ä¿å­˜æ‘˜è¦
        summary = {
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'successful_forums': successful_forums,
            'total_forums': len(forums),
            'total_matches': len(all_matches),
            'matches': all_matches
        }
        
        summary_file = os.path.join(self.base_dir, "monitoring_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

def main():
    try:
        bypasser = CloudflareBypasser()
        bypasser.run_monitoring()
    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()